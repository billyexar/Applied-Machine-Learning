# -*- coding: utf-8 -*-
"""assignment4_group-s.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1fcYKtYsE4Itu9B0OkKQ0n1LkC3Qv0a3Q
"""

'''
Files needed:
imdb.vocab
labeledBow.feat (from train folder)
labeledBow_test.feat (renamed from test folder)

Same dataset but in csv form:
from https://www.kaggle.com/datasets/atulanandjha/imdb-50k-movie-reviews-test-your-bert:
BERT_train.csv (renamed from train.csv)
BERT_test.csv (renamed from test.csv)

Make sure you have Runtime->Change runtime type->GPU
'''

"""## FILE PREP & SETUP"""

import urllib.request
import tarfile
urllib.request.urlretrieve("http://ai.stanford.edu/~amaas/data/sentiment/aclImdb_v1.tar.gz", "aclImdb_v1.tar.gz")
tar = tarfile.open("aclImdb_v1.tar.gz")
tar.extractall(".")
tar.close()

import shutil
shutil.move('/content/aclImdb/train/labeledBow.feat','/content')
shutil.move('/content/aclImdb/imdb.vocab','/content')

import os
os.chdir('/content/aclImdb/test')
os.rename('labeledBow.feat', 'labeledBow_test.feat')

os.chdir('/content')
shutil.move('/content/aclImdb/test/labeledBow_test.feat','/content')

import os
os.remove('aclImdb_v1.tar.gz')

shutil.rmtree('aclImdb')

from google.colab import drive
drive.mount('/content/drive')

import shutil
shutil.move('/content/drive/MyDrive/IMDB_Reviews/BERT_train.csv','/content')
shutil.move('/content/drive/MyDrive/IMDB_Reviews/BERT_test.csv','/content')

import os
# Print the current working directory
print(os.getcwd())

# Print the list of files in the current working directory
print(os.listdir())

# LET'S START THE JOURNEY!

"""## PART I"""

# Task 1: Data preprocessing
# 1.1 IMDB Reviews

import math
import pandas as pd
import numpy as np
import sklearn
from sklearn.datasets import load_svmlight_file
from scipy import sparse
import matplotlib.pyplot as plt

vocab_file_path = "/content/imdb.vocab"
vocab_file = open(vocab_file_path, "r")

review_data_file = open("/content/labeledBow.feat", "r")
review_data = review_data_file.read().split("\n")

word_list = vocab_file.read().split("\n") # create a list of all words in the vocab file

# initialize a dictionary to track the amount of occurrences of each word in the reviews
word_dict = {}
for word in word_list:
  word_dict[word] = 0

# count the total amount of reviews that each word appears in
for review in review_data:
  review_features = review.split(" ")
  for i in range (1, len(review_features)):
    word_index = int(review_features[i].split(":")[0])
    word_text = word_list[word_index]
    word_dict[word_text] += 1

print(word_dict)
print(f'{len(word_dict)} total words')

key_words = []

# remove any words that occur in over 50% of all reviews or under 1% of all reviews
for word in word_dict:
  if (word_dict[word] <= 12500 and word_dict[word] >= 250):
    key_words.append(word)

print(key_words)
print(f'{len(key_words)} key words')

# keep track of the the indices of the key words in the original word list
key_word_indices = []
for word in key_words:
  key_word_indices.append(word_list.index(word))

dataX, dataY = load_svmlight_file("/content/labeledBow.feat", dtype="uint8")

dataX = sparse.lil_matrix(sparse.csr_matrix(dataX)[:,key_word_indices]) # removing all columns for non-rare/stop words
dataY = sparse.lil_matrix(sparse.csr_matrix(dataY)[:,:])

dataX = dataX.toarray() # converting labeledBow.feat data into numpy arrays
dataY = dataY.toarray()

dataX = pd.DataFrame(dataX, columns=key_words)
dataY = pd.DataFrame(dataY.transpose(), columns=["Rating"])

z_scores = []

N = 25000

# standardizing all columns
# dataY = (dataY - dataY.mean()) / dataY.std()
# dataX = (dataX - dataX.mean()) / dataX.std()
for feature in dataX:
  dataX[feature] = (dataX[feature] - dataX[feature].mean()) / dataX[feature].std()

denominator = math.sqrt(N)

# calculating z scores for all key words
for word in key_words:
  z_scores.append(np.dot(dataX[word], dataY["Rating"]) / denominator)

# sorting z scores in order by magnitude
z_scores_sorted = sorted(z_scores, key=abs, reverse=True)

# saving the strongest key words for use in model testing
top_word_indices = []
numPos = 0
numNeg = 0

top_twenty = []
top_twenty_z = []

top_words = []

for i in range(200): # can change 200 to any value between 100 and 1000 for the amount of features to be used in testing
  top_word_index = key_word_indices[z_scores.index(z_scores_sorted[i])]
  top_word = key_words[z_scores.index(z_scores_sorted[i])]
  top_words.append(top_word)
  print(z_scores_sorted[i], top_word)
  top_word_indices.append(top_word_index)
  if (z_scores_sorted[i] > 0):
    if (numPos < 10):
      top_twenty.append(top_word)
      top_twenty_z.append(z_scores_sorted[i])
    numPos += 1
  else:
    if (numNeg < 10):
      top_twenty.append(top_word)
      top_twenty_z.append(z_scores_sorted[i])
    numNeg += 1

top_twenty.reverse()
top_twenty_z.reverse()

plt.barh(top_twenty, top_twenty_z)
plt.title('Top 20 features by z-score')
plt.ylabel('Word')
plt.xlabel('Z-Score')
plt.savefig("top_twenty_imdb.png")
plt.show()

print(top_word_indices)
print(f'number of positive features: {numPos}')
print(f'number of negative features: {numNeg}')

# get data

x_train, y_train = load_svmlight_file("/content/labeledBow.feat", dtype="uint8")

# using /aclImdb/test/labeledBow.feat as labeledBow_test.feat
x_test, y_test = load_svmlight_file("/content/labeledBow_test.feat", dtype="uint8")

x_train = sparse.lil_matrix(sparse.csr_matrix(x_train)[:,top_word_indices])
y_train = sparse.lil_matrix(sparse.csr_matrix(y_train)[:,:])

x_test = sparse.lil_matrix(sparse.csr_matrix(x_test)[:,top_word_indices])
y_test = sparse.lil_matrix(sparse.csr_matrix(y_test)[:,:])

x_train = x_train.toarray()
y_train = y_train.toarray()[0]

x_train = pd.DataFrame(x_train, columns=top_word_indices)
y_train = pd.DataFrame(y_train, columns=["Rating"])

x_test = x_test.toarray()
y_test = y_test.toarray()[0]

x_test = pd.DataFrame(x_test, columns=top_word_indices)
y_test = pd.DataFrame(y_test, columns=["Rating"])

def binary_rating(rating):
  if rating < 5:
    return 0
  else:
    return 1

# convert all ratings >= 5 to 1 and all ratings < 5 to 0 to enable binary classification
y_train["Rating"] = list(map(binary_rating, y_train["Rating"]))
y_test["Rating"] = list(map(binary_rating, y_test["Rating"]))

for feature in x_train:
  x_train[feature] = (x_train[feature] - x_train[feature].mean()) / x_train[feature].std()

for feature in x_test:
  x_test[feature] = (x_test[feature] - x_test[feature].mean()) / x_test[feature].std()

"""## PART II"""

# install pretrained bert
!pip install pytorch_pretrained_bert
!pip install pytorch-nlp

# Commented out IPython magic to ensure Python compatibility.
# get data for BERT

import sys
import numpy as np
import random as rn
import pandas as pd
import torch
from pytorch_pretrained_bert import BertModel
from torch import nn
from pytorch_pretrained_bert import BertTokenizer
from keras_preprocessing.sequence import pad_sequences
from torch.utils.data import TensorDataset, DataLoader, RandomSampler, SequentialSampler
from torch.optim import Adam
from torch.nn.utils import clip_grad_norm_
from IPython.display import clear_output
import matplotlib.pyplot as plt
from torchnlp.datasets import imdb_dataset
# %matplotlib inline


train_data = pd.read_csv('BERT_train.csv')
test_data = pd.read_csv('BERT_test.csv')
#train_data = imdb_dataset(train = True)
#test_data = imdb_dataset(test = True)


# using smaller size for training
train_data = train_data.sample(frac = 1)
train_data = train_data[:2000]


train_data = train_data.to_dict(orient='records')
test_data = test_data.to_dict(orient='records')

train_texts, train_labels = list(zip(*map(lambda d: (d['text'], d['sentiment']), train_data)))
test_texts, test_labels = list(zip(*map(lambda d: (d['text'], d['sentiment']), test_data)))

sentences = [len(sent) for sent in train_texts]

plt.rcParams.update({'figure.figsize':(7,5), 'figure.dpi':100})
plt.bar(range(1,2001), sentences, color = ['red'])
plt.ylim(0,6000)
plt.gca().set(title='No. of characters in each review', xlabel='Review Number', ylabel='Number of Characters');

print(train_texts[0])

sentences.sort()
counts, bins = np.histogram(sentences)
plt.clf()
plt.hist(bins[:-1], bins, weights=counts)
plt.xlabel('Number of Characters')
plt.ylabel('Number of Reviews')
plt.title('Histogram of Review Lengths')
plt.show()

# tokenize for BERT

tokenizer = BertTokenizer.from_pretrained('bert-base-uncased', do_lower_case=True)

train_tokens = list(map(lambda t: ['[CLS]'] + tokenizer.tokenize(t)[:510] + ['[SEP]'], train_texts))
test_tokens = list(map(lambda t: ['[CLS]'] + tokenizer.tokenize(t)[:510] + ['[SEP]'], test_texts))

train_tokens_ids = pad_sequences(list(map(tokenizer.convert_tokens_to_ids, train_tokens)), maxlen=512, truncating="post", padding="post", dtype="int")
test_tokens_ids = pad_sequences(list(map(tokenizer.convert_tokens_to_ids, test_tokens)), maxlen=512, truncating="post", padding="post", dtype="int")

train_tokens_ids.shape, test_tokens_ids.shape

train_y = np.array(train_labels) == 'pos'
test_y = np.array(test_labels) == 'pos'
train_y.shape, test_y.shape, np.mean(train_y), np.mean(test_y)

train_masks = [[float(i > 0) for i in ii] for ii in train_tokens_ids]
test_masks = [[float(i > 0) for i in ii] for ii in test_tokens_ids]

device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
device

class BertBinaryClassifier(nn.Module):
    def __init__(self, dropout=0.1):
        super(BertBinaryClassifier, self).__init__()

        self.bert = BertModel.from_pretrained('bert-base-uncased')

        self.dropout = nn.Dropout(dropout)
        self.linear = nn.Linear(768, 1)
        self.sigmoid = nn.Sigmoid()

    def forward(self, tokens, masks=None):
        _, pooled_output = self.bert(tokens, attention_mask=masks, output_all_encoded_layers=False)
        dropout_output = self.dropout(pooled_output)
        linear_output = self.linear(dropout_output)
        proba = self.sigmoid(linear_output)
        return proba

str(torch.cuda.memory_allocated(device)/1000000 ) + 'M'

bert_clf = BertBinaryClassifier()
bert_clf = bert_clf.cuda()

x = torch.tensor(train_tokens_ids[:3]).to(device)
y, pooled = bert_clf.bert(x, output_all_encoded_layers=False)
x.shape, y.shape, pooled.shape

y = bert_clf(x)
y.cpu().detach().numpy()        # kinda Garbage Collector to free up used and cache space

y, x, pooled = None, None, None
torch.cuda.empty_cache()

# Fine Tune BERT
BATCH_SIZE = 4
EPOCHS = 10

train_tokens_tensor = torch.tensor(train_tokens_ids)
train_y_tensor = torch.tensor(train_y.reshape(-1, 1)).float()

test_tokens_tensor = torch.tensor(test_tokens_ids)
test_y_tensor = torch.tensor(test_y.reshape(-1, 1)).float()

train_masks_tensor = torch.tensor(train_masks)
test_masks_tensor = torch.tensor(test_masks)

str(torch.cuda.memory_allocated(device)/1000000 ) + 'M'

train_dataset = TensorDataset(train_tokens_tensor, train_masks_tensor, train_y_tensor)
train_sampler = RandomSampler(train_dataset)
train_dataloader = DataLoader(train_dataset, sampler=train_sampler, batch_size=BATCH_SIZE)

test_dataset = TensorDataset(test_tokens_tensor, test_masks_tensor, test_y_tensor)
test_sampler = SequentialSampler(test_dataset)
test_dataloader = DataLoader(test_dataset, sampler=test_sampler, batch_size=BATCH_SIZE)

param_optimizer = list(bert_clf.sigmoid.named_parameters())
optimizer_grouped_parameters = [{"params": [p for n, p in param_optimizer]}]

optimizer = Adam(bert_clf.parameters(), lr=3e-6)

torch.cuda.empty_cache()

import time

start = time.time()

for epoch_num in range(EPOCHS):
    bert_clf.train()
    train_loss = 0
    for step_num, batch_data in enumerate(train_dataloader):
        token_ids, masks, labels = tuple(t.to(device) for t in batch_data)
        print(str(torch.cuda.memory_allocated(device)/1000000 ) + 'M')
        logits = bert_clf(token_ids, masks)

        loss_func = nn.BCELoss()

        batch_loss = loss_func(logits, labels)
        train_loss += batch_loss.item()


        bert_clf.zero_grad()
        batch_loss.backward()


        clip_grad_norm_(parameters=bert_clf.parameters(), max_norm=1.0)
        optimizer.step()

        clear_output(wait=True)
        print('Epoch: ', epoch_num + 1)
        print("\r" + "{0}/{1} loss: {2} ".format(step_num, len(train_data) / BATCH_SIZE, train_loss / (step_num + 1)))

end = time.time()
bert_train_time = end - start
print("BERT Train Time:", bert_train_time, "s")

from sklearn.metrics import classification_report

start = time.time()
bert_clf.eval()
bert_predicted = []
all_logits = []

with torch.no_grad():
    for step_num, batch_data in enumerate(test_dataloader):

        token_ids, masks, labels = tuple(t.to(device) for t in batch_data)

        logits = bert_clf(token_ids, masks)
        loss_func = nn.BCELoss()
        loss = loss_func(logits, labels)
        numpy_logits = logits.cpu().detach().numpy()

        bert_predicted += list(numpy_logits[:, 0] > 0.5)
        all_logits += list(numpy_logits[:, 0])

end = time.time()
bert_predict_time = end - start

print(classification_report(test_y, bert_predicted))
print("BERT Predict Time:", bert_predict_time, "s")

import tensorflow as tf

#print(test_y)
bert_y_test = []
for i in range(len(test_y)):
  if test_y[i]:
    bert_y_test.append(1)
  else:
    bert_y_test.append(0)

from sklearn.tree import DecisionTreeClassifier
from sklearn.neighbors import KNeighborsClassifier
from sklearn.linear_model import LogisticRegression
from sklearn.svm import SVC
from sklearn.pipeline import make_pipeline
from sklearn.preprocessing import StandardScaler
from sklearn.metrics import confusion_matrix
from sklearn.metrics import roc_curve, roc_auc_score
from sklearn.ensemble import GradientBoostingClassifier
from sklearn.ensemble import RandomForestClassifier

#SVC(kernel='linear', probability=True)
models = [KNeighborsClassifier(),
          DecisionTreeClassifier(),
          LogisticRegression(),
          GradientBoostingClassifier(),
          RandomForestClassifier(),
          make_pipeline(StandardScaler(), SVC(kernel='linear', probability=True))
          ]

perf = {}

# BERT

fpr, tpr, _ = roc_curve(bert_y_test, all_logits)
auroc = roc_auc_score(bert_y_test, all_logits)
perf["BERT"] = {'fpr': fpr, 'tpr': tpr, 'auroc': auroc}

#############
fit_times = []
predict_times = []

for model in models:

    start = time.time()
    fit = model.fit(x_train, y_train["Rating"])
    end = time.time()
    fit_times.append(end-start)

    start = time.time()
    y_test_prob = fit.predict_proba(x_test)[:,1]
    end = time.time()
    predict_times.append(end-start)

    fpr, tpr, _ = roc_curve(y_test, y_test_prob)
    auroc = roc_auc_score(y_test, y_test_prob)
    perf[type(model).__name__] = {'fpr': fpr,'tpr': tpr,'auroc': auroc}


plt.clf()
i = 0
for model_name, model_perf in perf.items():
    plt.plot(model_perf['fpr'], model_perf['tpr'],label=model_name)
    plt.text(0.4, i+0.1, model_name + ': AUC = '+ str(round(model_perf['auroc'],2)))
    i += 0.1

plt.plot([0, 1], [0, 1], color="navy", lw=2, linestyle="--")
plt.xlim([0.0, 1.0])
plt.ylim([0.0, 1.05])

plt.xlabel("False Positive Rate")
plt.ylabel("True Positive Rate")
plt.title('ROC in predicting IMDB reviews')
plt.legend(bbox_to_anchor=(1.04, 0.5), loc="upper left")
plt.savefig("imdb_roc_curve.png", bbox_inches='tight', dpi=300)
plt.show()
# plt.close()

plt.clf()
i = 0
for model_name, model_perf in perf.items():
    if model_name == "Pipeline":
      model_name = "SVM"
    plt.plot(model_perf['fpr'], model_perf['tpr'],label=model_name)
    #plt.text(0.4, i+0.1, model_name + ': AUC = '+ str(round(model_perf['auroc'],4)))
    i += 0.1

plt.plot([0, 1], [0, 1], color="navy", lw=2, linestyle="--")
plt.xlim([0.0, 1.0])
plt.ylim([0.0, 1.05])

plt.xlabel("False Positive Rate")
plt.ylabel("True Positive Rate")
plt.title('ROC in predicting IMDB reviews')
plt.legend(bbox_to_anchor=(1.04, 0.5), loc="upper left")
plt.savefig("imdb_roc_curve.png", bbox_inches='tight', dpi=300)
plt.show()
# plt.close()

# bar plot of AUROC scores
plt.clf()

plot_x = []
plot_y = []
i=0
for model_name, model_perf in perf.items():
    plot_x.append(round(model_perf['auroc'],2))
    if model_name == "Pipeline":
      plot_y.append("SVM")
    else:
      plot_y.append(model_name)
    plt.text(0.4, i, str(round(model_perf['auroc'],6)))
    i += 1

plt.barh(plot_y, plot_x)

plt.xlabel("AUROC Score")
plt.ylabel("Model")
plt.title('AUROC Score by Model')
plt.legend(bbox_to_anchor=(1.04, 0.5), loc="upper left")
plt.savefig("imdb_auroc_scores.png", bbox_inches='tight', dpi=300)
plt.show()
# plt.close()

models = ["BERT"] + models
fit_times = [bert_train_time] + fit_times
predict_times = [bert_predict_time] + predict_times

for i in range(len(models)):
    print(models[i], "---- Train Time:", fit_times[i], "s ----", "Predict Time:", predict_times[i], "s")

# Random Forest Feature Importance
from sklearn.ensemble import RandomForestClassifier

feature_names = top_words
forest = RandomForestClassifier(random_state=0)
forest.fit(x_train, y_train["Rating"])

from sklearn.inspection import permutation_importance

start_time = time.time()
result = permutation_importance(
    forest, x_test, y_test, n_repeats=10, random_state=42, n_jobs=2
)
elapsed_time = time.time() - start_time
print(f"Elapsed time to compute the importances: {elapsed_time:.3f} seconds")

forest_importances = pd.Series(result.importances_mean, index=feature_names)

forest_importances.sort_values(ascending=False, inplace=True)

top_importances = forest_importances.iloc[:20]

fig, ax = plt.subplots()
top_importances.plot.barh(ax=ax)
ax.set_title("Random Forest Feature importances Using Permutation on Full Model")
ax.set_xlabel("Mean accuracy decrease")
fig.tight_layout()
plt.show()

# Plot top 20 features for RF using feature permutationÂ¶
import time

start_time = time.time()
importances = forest.feature_importances_
std = np.std([tree.feature_importances_ for tree in forest.estimators_], axis=0)
elapsed_time = time.time() - start_time

print(f"Elapsed time to compute the importances: {elapsed_time:.3f} seconds")

forest_importances = pd.Series(importances, index=feature_names)
forest_importances.sort_values(ascending=False, inplace=True)

top_importances = forest_importances.iloc[:20]

fig, ax = plt.subplots()
top_importances.plot.barh(ax=ax)
ax.set_title("Random Forest Feature Importances Using MDI")
ax.set_xlabel("Mean decrease in impurity")
fig.tight_layout()
plt.show()

"""## Attention Matrices for Correctly Predicted Positive and Negative Reviews"""

!pip install transformers

from transformers import BertModel, BertTokenizer
import torch
model = BertModel.from_pretrained("bert-base-uncased",output_attentions=True)
input = torch.tensor(test_tokens_ids[:10]).cpu()
output = model(input)

# Attention Retrieval
attention_weights = output[-1] # extract 12 multi-headed attention weights from predicted outputs
attention = attention_weights[0].cpu().detach().numpy() # choose the 1st head's attention
attention_to_use = attention[0,0,:,:] # [batch_size, num_heads, sequence_length, sequence_length]

test_y[:10]

bert_predicted[:10]

import seaborn as sns
import matplotlib.pyplot as plt

words = tokenizer.convert_ids_to_tokens(test_tokens_ids[4,:]) # a correctly predicted POSITIVE review
fig, ax = plt.subplots(figsize=(10, 10))
sns.heatmap(attention_to_use, xticklabels=words, yticklabels=words)
plt.title('Attention Matrix for a Correctly Predicted Positive Review')
plt.xticks(rotation=45)
plt.yticks(rotation=45)
ax = plt.gca()
ax.set_xlim(0,10)
ax.set_ylim(0,10)
plt.xlabel("Input Words", fontsize=20)
plt.ylabel("Output Words", fontsize=20)
ax.set_xticklabels(ax.get_xticklabels(), fontsize=20)
ax.set_yticklabels(ax.get_yticklabels(), fontsize=20)
plt.show()

words = tokenizer.convert_ids_to_tokens(test_tokens_ids[2,:]) # a correctly predicted NEGATIVE review
fig, ax = plt.subplots(figsize=(10, 10))
sns.heatmap(attention_to_use, xticklabels=words, yticklabels=words)
plt.title('Attention Matrix for a Correctly Predicted Negative Review')
plt.xticks(rotation=45)
plt.yticks(rotation=45)
ax = plt.gca()
ax.set_xlim(0,10)
ax.set_ylim(0,10)
plt.xlabel("Input Words", fontsize=20)
plt.ylabel("Output Words", fontsize=20)
ax.set_xticklabels(ax.get_xticklabels(), fontsize=20)
ax.set_yticklabels(ax.get_yticklabels(), fontsize=20)
plt.show()

words = tokenizer.convert_ids_to_tokens(test_tokens_ids[5,:]) # an incorrectly predicted review
fig, ax = plt.subplots(figsize=(10, 10))
sns.heatmap(attention_to_use, xticklabels=words, yticklabels=words)
plt.title('Attention Matrix for an Incorrectly Predicted Review')
plt.xticks(rotation=45)
plt.yticks(rotation=45)
ax = plt.gca()
ax.set_xlim(0,10)
ax.set_ylim(0,10)
plt.xlabel("Input Words", fontsize=20)
plt.ylabel("Output Words", fontsize=20)
ax.set_xticklabels(ax.get_xticklabels(), fontsize=20)
ax.set_yticklabels(ax.get_yticklabels(), fontsize=20)
plt.show()